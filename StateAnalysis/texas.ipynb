{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 14:58:00,798 - INFO - Starting analysis for Texas (TX)\n",
      "2025-05-12 14:58:00,798 - INFO - Loading voter file: ../StateData/TexasData/TX_2022_prim_l2_vf_2020blocks/TX_2022_prim_l2_vf_2020blocks.csv\n",
      "/var/folders/z9/gzbgl18s2wgcr125qq0q2h280000gn/T/ipykernel_581/944456676.py:283: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['480019501001000' '480019501001001' '480019501001002' ...\n",
      " '485079503025022' '485079503025023' '485079503025024']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_active.loc[:, VF_BLOCK_ID_COL] = df_active[VF_BLOCK_ID_COL].astype(str).str.zfill(15)\n",
      "2025-05-12 14:58:05,797 - INFO - Aggregating voter data to Census Tracts...\n",
      "2025-05-12 14:58:05,853 - INFO - Aggregated voter data for 6874 tracts.\n",
      "2025-05-12 14:58:05,854 - INFO - Loading CVAP data: ../StateData/TexasData/tx_cvap_2022_2020_b/tx_cvap_2022_2020_b.csv\n",
      "/var/folders/z9/gzbgl18s2wgcr125qq0q2h280000gn/T/ipykernel_581/944456676.py:324: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['482019802001045' '484739800001001' '481130100021009' ...\n",
      " '480019501001006' '480019501001004' '480019501001003']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_cvap.loc[:, CVAP_BLOCK_ID_COL] = df_cvap[CVAP_BLOCK_ID_COL].astype(str).str.zfill(15)\n",
      "2025-05-12 14:58:06,598 - INFO - Aggregating CVAP data to Census Tracts and calculating percentages...\n",
      "2025-05-12 14:58:06,627 - INFO - Aggregated CVAP data for 6852 tracts.\n",
      "2025-05-12 14:58:06,627 - INFO - Loading Tract shapefile: ../StateData/TexasData/tx_t_2020_bound/tx_t_2020_bound.shp\n",
      "2025-05-12 14:58:06,850 - INFO - Loading District shapefile: ../StateData/TexasData/tx_cong_2021/PLANC2193.shp\n",
      "2025-05-12 14:58:06,969 - INFO - Merging tract-level data...\n",
      "2025-05-12 14:58:06,980 - INFO - Merged data for 6874 tracts with geometries.\n",
      "2025-05-12 14:58:06,981 - INFO - Performing spatial join (assigning tracts to districts)...\n",
      "2025-05-12 14:58:06,983 - INFO - Reprojecting tracts from EPSG:4269 to PROJCS[\"NAD_1983_Lambert_Conformal_Conic\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"latitude_of_origin\",31.1666666666667],PARAMETER[\"central_meridian\",-100],PARAMETER[\"standard_parallel_1\",27.4166666666667],PARAMETER[\"standard_parallel_2\",34.9166666666667],PARAMETER[\"false_easting\",1000000],PARAMETER[\"false_northing\",1000000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]] for spatial join.\n",
      "2025-05-12 14:58:10,131 - INFO - Assigned 6874 tracts to districts.\n",
      "2025-05-12 14:58:10,132 - INFO - Aggregating data to district level...\n",
      "2025-05-12 14:58:10,137 - INFO - Calculating Polsby-Popper scores...\n",
      "2025-05-12 14:58:10,144 - WARNING - Error reprojecting for Polsby-Popper: Invalid projection: EPSG:XXXXX: (Internal Proj Error: proj_create: crs not found). Using original CRS (may be less accurate).\n",
      "2025-05-12 14:58:10,149 - INFO - Calculating Efficiency Gap...\n",
      "2025-05-12 14:58:10,152 - INFO - Calculating Community Concentration (Threshold: >30.0%) ...\n",
      "2025-05-12 14:58:10,154 - INFO - Analyzing: Black (using pct_black) - Found 982 tracts statewide >30.0% CVAP.\n",
      "2025-05-12 14:58:10,160 - INFO - Analyzing: Hispanic (using pct_hispanic) - Found 2810 tracts statewide >30.0% CVAP.\n",
      "2025-05-12 14:58:10,163 - INFO - Analyzing: Asian (using pct_asian) - Found 113 tracts statewide >30.0% CVAP.\n",
      "2025-05-12 14:58:10,171 - INFO - \n",
      "--- METRIC SUMMARY ---\n",
      "2025-05-12 14:58:10,172 - INFO - State: Texas\n",
      "2025-05-12 14:58:10,174 - INFO - \n",
      "Polsby-Popper Scores:\n",
      " District  polsby_popper\n",
      "        1         0.1563\n",
      "        2         0.2277\n",
      "        3         0.3395\n",
      "        4         0.0760\n",
      "        5         0.1468\n",
      "        6         0.1537\n",
      "        7         0.0918\n",
      "        8         0.2247\n",
      "        9         0.1639\n",
      "       10         0.1850\n",
      "       11         0.3054\n",
      "       12         0.2080\n",
      "       13         0.2798\n",
      "       14         0.1610\n",
      "       15         0.1115\n",
      "       16         0.2297\n",
      "       17         0.1374\n",
      "       18         0.0678\n",
      "       19         0.5317\n",
      "       20         0.1292\n",
      "       21         0.3050\n",
      "       22         0.1637\n",
      "       23         0.1970\n",
      "       24         0.1141\n",
      "       25         0.2588\n",
      "       26         0.1493\n",
      "       27         0.3687\n",
      "       28         0.2088\n",
      "       29         0.0918\n",
      "       30         0.1967\n",
      "       31         0.1975\n",
      "       32         0.0769\n",
      "       33         0.0378\n",
      "       34         0.2671\n",
      "       35         0.0783\n",
      "       36         0.2481\n",
      "       37         0.1539\n",
      "       38         0.1246\n",
      "2025-05-12 14:58:10,175 - INFO - \n",
      "Efficiency Gap: -0.1897\n",
      "2025-05-12 14:58:10,175 - INFO - Interpretation: Favors Republicans by 18.97%\n",
      "2025-05-12 14:58:10,175 - INFO - \n",
      "Community Concentration Summary:\n",
      "2025-05-12 14:58:10,176 - INFO - \n",
      "Minority Group  Total High-Conc Tracts Statewide Concentration Score (Sum Sq Shares) Baseline Even Spread                                Interpretation\n",
      "         Black                               982                               0.064                0.026 Dispersed / Potential Cracking (Score: 0.064)\n",
      "      Hispanic                              2810                               0.045                0.026 Dispersed / Potential Cracking (Score: 0.045)\n",
      "         Asian                               113                               0.131                0.026 Dispersed / Potential Cracking (Score: 0.131)\n",
      "2025-05-12 14:58:10,181 - INFO - Saved state summary JSON to ../results/TX/\n",
      "2025-05-12 14:58:11,138 - INFO - Created 38 records\n",
      "2025-05-12 14:58:11,139 - INFO - Saved detailed district metrics to GeoJSON in ../results/TX/\n",
      "2025-05-12 14:58:11,313 - INFO - Saved compactness map to ../results/TX/\n",
      "2025-05-12 14:58:11,313 - INFO - Analysis for Texas complete. Results in ../results/TX/\n"
     ]
    }
   ],
   "source": [
    "# === Generic State Gerrymandering Analysis Script Template ===\n",
    "#\n",
    "# Instructions for use:\n",
    "# 1. Copy this entire file and rename it for the state you are analyzing\n",
    "#    (e.g., analyze_newyork.py, analyze_texas.py).\n",
    "# 2. VERY CAREFULLY review and update ALL variables in the\n",
    "#    \"--- STATE-SPECIFIC CONFIGURATION - UPDATE FOR EACH STATE ---\" section.\n",
    "#    This is the MOST CRITICAL step. Errors here will cause the script to fail\n",
    "#    or produce incorrect results.\n",
    "# 3. Ensure all data files listed in the paths exist and are accessible.\n",
    "# 4. Run the script from your terminal: python analyze_yourstate.py\n",
    "#\n",
    "# Outputs will be saved to a 'results/STATE_ABBR/' directory.\n",
    "#\n",
    "# Required libraries: pandas, geopandas, numpy, matplotlib\n",
    "# Ensure they are installed: pip install pandas geopandas numpy matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# --- Configure Logging ---\n",
    "# Clears previous handlers to avoid duplicate messages if re-running in a notebook\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# --- STATE-SPECIFIC CONFIGURATION - UPDATE FOR EACH STATE ---\n",
    "# V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V\n",
    "#                          MUST BE UPDATED FOR EACH STATE\n",
    "# V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V\n",
    "\n",
    "STATE_ABBR = \"TX\"  # e.g., \"NY\", \"TX\" - Used for creating output folder names\n",
    "STATE_NAME = \"Texas\"  # e.g., \"New York\", \"Texas\"\n",
    "\n",
    "# --- File Paths ---\n",
    "# (Adjust these paths relative to where you run the script, or use absolute paths)\n",
    "# It's recommended to have a base data directory and then state-specific subdirectories.\n",
    "# Example: project_root/data/NY/voter_files/NY_voter_file.csv\n",
    "BASE_DATA_PATH = f\"../StateData/TexasData/\" # Adjusted for running from StateAnalysis folder\n",
    "VOTER_FILE_PATH = os.path.join(BASE_DATA_PATH, \"TX_2022_prim_l2_vf_2020blocks/TX_2022_prim_l2_vf_2020blocks.csv\") # VERIFY THIS FILENAME\n",
    "CVAP_FILE_PATH = os.path.join(BASE_DATA_PATH, \"tx_cvap_2022_2020_b/tx_cvap_2022_2020_b.csv\") # VERIFY THIS FILENAME\n",
    "DISTRICT_SHAPEFILE_PATH = os.path.join(BASE_DATA_PATH, \"tx_cong_2021/PLANC2193.shp\") # VERIFY THIS FILENAME (e.g., Plan C2185 or similar)\n",
    "TRACT_SHAPEFILE_PATH = os.path.join(BASE_DATA_PATH, \"tx_t_2020_bound/tx_t_2020_bound.shp\") # VERIFY THIS FILENAME\n",
    "\n",
    "# --- Target CRS for Geometric Calculations (Area, Perimeter) ---\n",
    "# Find a suitable projected CRS for the state (e.g., State Plane or UTM zone)\n",
    "# Search for \"[State Name] projected CRS EPSG\"\n",
    "TARGET_CRS_EPSG = \"EPSG:XXXXX\" # e.g., \"EPSG:32118\" for NY Long Island (State Plane)\n",
    "\n",
    "# --- Key Column Names (VERIFY THESE AGAINST YOUR ACTUAL FILES) ---\n",
    "# Voter File Columns\n",
    "VF_BLOCK_ID_COL = 'geoid20'         # 15-digit block GEOID in voter file (or equivalent)\n",
    "VF_TOTAL_REG_COL = 'total_reg'\n",
    "VF_PARTY_DEM_COL = 'party_dem'\n",
    "VF_PARTY_REP_COL = 'party_rep'\n",
    "VF_PARTY_NPP_COL = 'party_npp'      # Non-Partisan/Other - use if available, otherwise script can handle its absence\n",
    "VF_GENERAL_2020_VOTES_COL = 'g20201103' # L2 column for 2020 General Election turnout (or relevant election)\n",
    "# Add any specific election result columns if needed for renaming for your state\n",
    "# Example: 'p20220524': 'Primary_2022'\n",
    "VF_ELECTION_RENAME_DICT = {\n",
    "    'g20201103': 'General_2020_Turnout' # Example, ensure VF_GENERAL_2020_VOTES_COL matches a key here if renaming\n",
    "    # Add other election renames specific to this state's L2 file if you use them\n",
    "}\n",
    "\n",
    "\n",
    "# CVAP File Columns\n",
    "CVAP_BLOCK_ID_COL = 'GEOID20'       # 15-digit block GEOID in CVAP file\n",
    "CVAP_TOTAL_COL = 'CVAP_TOT22'\n",
    "CVAP_WHITE_COL = 'CVAP_WHT22'\n",
    "CVAP_BLACK_COL = 'CVAP_BLK22'\n",
    "CVAP_HISPANIC_COL = 'CVAP_HSP22'\n",
    "CVAP_ASIAN_COL = 'CVAP_ASN22'\n",
    "CVAP_AIAN_COL = 'CVAP_AIA22'        # American Indian/Alaska Native\n",
    "CVAP_NHPI_COL = 'CVAP_NHP22'        # Native Hawaiian/Pacific Islander\n",
    "# Add other CVAP groups if needed, e.g., CVAP_2OM22 (Two or More Races)\n",
    "\n",
    "# Tract Shapefile Columns\n",
    "TRACT_SHP_ID_COL = 'GEOID20'        # Typically the 11-digit tract GEOID in tract shapefile\n",
    "\n",
    "# District Shapefile Columns\n",
    "DISTRICT_SHP_ID_COL = 'District'    # Or 'CD118FP', 'DISTRICTNO', 'GEOID', etc. VERIFY!\n",
    "\n",
    "# --- Community Concentration Analysis Parameters ---\n",
    "COMMUNITY_THRESHOLD_PCT = 30.0\n",
    "# Define which minority groups to analyze and their corresponding CVAP percentage column names\n",
    "# (these 'pct_...' columns will be created during CVAP processing)\n",
    "MINORITY_GROUPS_FOR_ANALYSIS = {\n",
    "    # Internal key : Name of the 'pct_group' column that will be created\n",
    "    'black': 'pct_black',\n",
    "    'hispanic': 'pct_hispanic',\n",
    "    'asian': 'pct_asian',\n",
    "    # 'aian': 'pct_aian', # Uncomment and ensure CVAP_AIAN_COL is set if you want to analyze\n",
    "    # 'nhpi': 'pct_nhpi'  # Uncomment and ensure CVAP_NHPI_COL is set\n",
    "}\n",
    "# Map these internal keys to the raw CVAP count column names from your CVAP file\n",
    "# This helps in aggregating raw counts to districts and calculating overall district %\n",
    "CVAP_COLUMN_MAP_FOR_GROUPS = {\n",
    "    'black': CVAP_BLACK_COL,\n",
    "    'hispanic': CVAP_HISPANIC_COL,\n",
    "    'asian': CVAP_ASIAN_COL,\n",
    "    'aian': CVAP_AIAN_COL,\n",
    "    'nhpi': CVAP_NHPI_COL\n",
    "    # Add other groups if defined in MINORITY_GROUPS_FOR_ANALYSIS\n",
    "}\n",
    "\n",
    "# ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
    "#                          END OF STATE-SPECIFIC CONFIGURATION\n",
    "# ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
    "\n",
    "# --- Output Directory ---\n",
    "RESULTS_DIR = f\"../results/{STATE_ABBR}/\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# --- Helper Functions (Should not need modification for different states) ---\n",
    "\n",
    "def calculate_polsby_popper(gdf_districts_input, district_id_col, target_crs):\n",
    "    \"\"\"Calculates Polsby-Popper scores.\"\"\"\n",
    "    logging.info(\"Calculating Polsby-Popper scores...\")\n",
    "    gdf_districts_scores = gdf_districts_input.copy()\n",
    "    try:\n",
    "        gdf_districts_proj = gdf_districts_scores.to_crs(target_crs)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error reprojecting for Polsby-Popper: {e}. Using original CRS (may be less accurate).\")\n",
    "        gdf_districts_proj = gdf_districts_scores\n",
    "        \n",
    "    gdf_districts_proj['area'] = gdf_districts_proj.geometry.area\n",
    "    gdf_districts_proj['perimeter'] = gdf_districts_proj.geometry.length\n",
    "    gdf_districts_proj['polsby_popper'] = 0.0\n",
    "    valid_perimeter = gdf_districts_proj['perimeter'] > 0\n",
    "    # Avoid division by zero or issues with very small perimeters\n",
    "    gdf_districts_proj.loc[valid_perimeter, 'polsby_popper'] = \\\n",
    "        (4 * np.pi * gdf_districts_proj.loc[valid_perimeter, 'area']) / \\\n",
    "        (gdf_districts_proj.loc[valid_perimeter, 'perimeter']**2).replace(0, np.nan) # Avoid division by zero\n",
    "    gdf_districts_scores['polsby_popper'] = gdf_districts_proj['polsby_popper'].fillna(0)\n",
    "    return gdf_districts_scores[[district_id_col, 'polsby_popper']]\n",
    "\n",
    "def calculate_efficiency_gap(gdf_districts_input, district_id_col, dem_votes_col, rep_votes_col):\n",
    "    \"\"\"Calculates the Efficiency Gap.\"\"\"\n",
    "    logging.info(\"Calculating Efficiency Gap...\")\n",
    "    if dem_votes_col not in gdf_districts_input.columns or rep_votes_col not in gdf_districts_input.columns:\n",
    "        logging.error(f\"Vote columns ('{dem_votes_col}', '{rep_votes_col}') not found in district data for EG calc.\")\n",
    "        return np.nan, pd.DataFrame(columns=['DISTRICT', 'Dem_Votes', 'Rep_Votes', 'Total_Votes', 'Wasted_Dem_Votes', 'Wasted_Rep_Votes'])\n",
    "\n",
    "    df_results = gdf_districts_input[[district_id_col, dem_votes_col, rep_votes_col]].copy()\n",
    "    df_results.rename(columns={district_id_col: 'DISTRICT', dem_votes_col: 'Dem_Votes', rep_votes_col: 'Rep_Votes'}, inplace=True)\n",
    "\n",
    "    df_results['Total_Votes'] = df_results['Dem_Votes'] + df_results['Rep_Votes']\n",
    "    df_results['Win_Threshold'] = np.floor(df_results['Total_Votes'] / 2) + 1\n",
    "    df_results['Wasted_Dem_Votes'] = 0.0\n",
    "    df_results['Wasted_Rep_Votes'] = 0.0\n",
    "\n",
    "    dem_wins = df_results['Dem_Votes'] > df_results['Rep_Votes']\n",
    "    rep_wins = df_results['Rep_Votes'] > df_results['Dem_Votes']\n",
    "    tie = df_results['Dem_Votes'] == df_results['Rep_Votes']\n",
    "\n",
    "    df_results.loc[dem_wins, 'Wasted_Dem_Votes'] = df_results['Dem_Votes'] - df_results['Win_Threshold']\n",
    "    df_results.loc[rep_wins | tie, 'Wasted_Dem_Votes'] = df_results['Dem_Votes']\n",
    "    df_results.loc[rep_wins, 'Wasted_Rep_Votes'] = df_results['Rep_Votes'] - df_results['Win_Threshold']\n",
    "    df_results.loc[dem_wins | tie, 'Wasted_Rep_Votes'] = df_results['Rep_Votes']\n",
    "\n",
    "    df_results['Wasted_Dem_Votes'] = df_results['Wasted_Dem_Votes'].clip(lower=0)\n",
    "    df_results['Wasted_Rep_Votes'] = df_results['Wasted_Rep_Votes'].clip(lower=0)\n",
    "\n",
    "    total_votes_statewide = df_results['Total_Votes'].sum()\n",
    "    total_wasted_dem = df_results['Wasted_Dem_Votes'].sum()\n",
    "    total_wasted_rep = df_results['Wasted_Rep_Votes'].sum()\n",
    "\n",
    "    efficiency_gap_value = (total_wasted_rep - total_wasted_dem) / total_votes_statewide if total_votes_statewide > 0 else 0\n",
    "    return efficiency_gap_value, df_results\n",
    "\n",
    "def calculate_community_concentration(gdf_tracts_with_district_input, district_id_col, minority_pct_cols_map, threshold_pct):\n",
    "    \"\"\"Calculates community concentration.\"\"\"\n",
    "    logging.info(f\"Calculating Community Concentration (Threshold: >{threshold_pct}%) ...\")\n",
    "    gdf_tracts_with_district = gdf_tracts_with_district_input.copy()\n",
    "    concentration_results = []\n",
    "    distribution_details_list = []\n",
    "    \n",
    "    if district_id_col not in gdf_tracts_with_district.columns:\n",
    "        logging.error(f\"District ID column '{district_id_col}' not found in input GeoDataFrame for comm. conc.\")\n",
    "        return pd.DataFrame(), pd.DataFrame() # Return empty DataFrames\n",
    "        \n",
    "    num_districts = gdf_tracts_with_district[district_id_col].nunique()\n",
    "    if num_districts == 0:\n",
    "        logging.warning(\"No districts found. Skipping community concentration.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    baseline_even_spread = 1 / num_districts\n",
    "\n",
    "    all_districts = sorted(gdf_tracts_with_district[district_id_col].unique())\n",
    "    district_summary_base = pd.DataFrame({district_id_col: all_districts})\n",
    "\n",
    "    for group_key, pct_col_name in minority_pct_cols_map.items():\n",
    "        group_name_display = group_key.capitalize()\n",
    "        high_conc_col = f'high_{group_key}_cvap_{threshold_pct}'\n",
    "\n",
    "        if pct_col_name not in gdf_tracts_with_district.columns:\n",
    "            logging.warning(f\"CVAP percentage column '{pct_col_name}' for group '{group_name_display}' not found. Skipping this group.\")\n",
    "            concentration_results.append({\n",
    "                'Minority Group': group_name_display, 'Threshold (%)': threshold_pct,\n",
    "                'Total High-Conc Tracts Statewide': 0, 'Concentration Score (Sum Sq Shares)': np.nan,\n",
    "                'Baseline Even Spread': baseline_even_spread, 'Interpretation': 'Skipped - Data missing'\n",
    "            })\n",
    "            empty_df = district_summary_base.copy()\n",
    "            empty_df[f'high_{group_key}_tracts'] = 0\n",
    "            empty_df[f'share_statewide_high_{group_key}'] = 0.0\n",
    "            distribution_details_list.append(empty_df[[district_id_col, f'high_{group_key}_tracts', f'share_statewide_high_{group_key}']])\n",
    "            continue\n",
    "\n",
    "        gdf_tracts_with_district.loc[:, high_conc_col] = gdf_tracts_with_district[pct_col_name] > threshold_pct\n",
    "        total_high_conc_tracts_statewide = gdf_tracts_with_district[high_conc_col].sum()\n",
    "        logging.info(f\"Analyzing: {group_name_display} (using {pct_col_name}) - Found {total_high_conc_tracts_statewide} tracts statewide >{threshold_pct}% CVAP.\")\n",
    "\n",
    "        high_tracts_col_name = f'high_{group_key}_tracts'\n",
    "        share_col_name = f'share_statewide_high_{group_key}'\n",
    "\n",
    "        if total_high_conc_tracts_statewide > 0:\n",
    "            dist_group_summary = gdf_tracts_with_district[gdf_tracts_with_district[high_conc_col]].groupby(district_id_col).size().reset_index(name=high_tracts_col_name)\n",
    "            dist_group_summary = pd.merge(district_summary_base, dist_group_summary, on=district_id_col, how='left').fillna(0)\n",
    "            dist_group_summary[high_tracts_col_name] = dist_group_summary[high_tracts_col_name].astype(int)\n",
    "            dist_group_summary[share_col_name] = dist_group_summary[high_tracts_col_name] / total_high_conc_tracts_statewide\n",
    "            \n",
    "            concentration_score = (dist_group_summary[share_col_name]**2).sum()\n",
    "            \n",
    "            mid_point = (baseline_even_spread + 1.0) / 2\n",
    "            lower_bound = baseline_even_spread + (mid_point - baseline_even_spread) * 0.33 # Adjusted thresholds slightly\n",
    "            upper_bound = mid_point + (1.0 - mid_point) * 0.33 # Adjusted thresholds slightly\n",
    "\n",
    "            if concentration_score >= upper_bound : interpretation = f\"High Concentration / Packing (Score: {concentration_score:.3f})\"\n",
    "            elif concentration_score <= lower_bound: interpretation = f\"Dispersed / Potential Cracking (Score: {concentration_score:.3f})\"\n",
    "            else: interpretation = f\"Moderate Concentration / Uneven Split (Score: {concentration_score:.3f})\"\n",
    "        else:\n",
    "            concentration_score = np.nan\n",
    "            interpretation = \"No tracts above threshold\"\n",
    "            dist_group_summary = district_summary_base.copy()\n",
    "            dist_group_summary[high_tracts_col_name] = 0\n",
    "            dist_group_summary[share_col_name] = 0.0\n",
    "            \n",
    "        distribution_details_list.append(dist_group_summary[[district_id_col, high_tracts_col_name, share_col_name]])\n",
    "        concentration_results.append({\n",
    "            'Minority Group': group_name_display, 'Threshold (%)': threshold_pct,\n",
    "            'Total High-Conc Tracts Statewide': total_high_conc_tracts_statewide,\n",
    "            'Concentration Score (Sum Sq Shares)': concentration_score,\n",
    "            'Baseline Even Spread': baseline_even_spread, 'Interpretation': interpretation\n",
    "        })\n",
    "        gdf_tracts_with_district.drop(columns=[high_conc_col], inplace=True, errors='ignore')\n",
    "\n",
    "    final_distribution_df = district_summary_base.copy()\n",
    "    for detail_df in distribution_details_list:\n",
    "        # Ensure the detail_df has the district_id_col before merging\n",
    "        if district_id_col in detail_df.columns:\n",
    "            final_distribution_df = pd.merge(final_distribution_df, detail_df, on=district_id_col, how='left')\n",
    "        else:\n",
    "            logging.warning(f\"Skipping merge for a detail_df as it's missing district_id_col: {district_id_col}\")\n",
    "\n",
    "    return pd.DataFrame(concentration_results), final_distribution_df.fillna(0)\n",
    "\n",
    "# --- Main Analysis Logic ---\n",
    "def main():\n",
    "    global DISTRICT_SHP_ID_COL\n",
    "    logging.info(f\"Starting analysis for {STATE_NAME} ({STATE_ABBR})\")\n",
    "\n",
    "    # 1. Load Voter File Data\n",
    "    logging.info(f\"Loading voter file: {VOTER_FILE_PATH}\")\n",
    "    try:\n",
    "        df_vf = pd.read_csv(VOTER_FILE_PATH, low_memory=False) # low_memory=False can help with mixed types\n",
    "        # Apply renames if dict is provided and not empty\n",
    "        if VF_ELECTION_RENAME_DICT:\n",
    "            df_vf.rename(columns=VF_ELECTION_RENAME_DICT, inplace=True)\n",
    "            # Update VF_GENERAL_2020_VOTES_COL if it was renamed\n",
    "            global VF_GENERAL_2020_VOTES_COL \n",
    "            VF_GENERAL_2020_VOTES_COL = VF_ELECTION_RENAME_DICT.get(VF_GENERAL_2020_VOTES_COL, VF_GENERAL_2020_VOTES_COL)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Voter file not found at {VOTER_FILE_PATH}. Please check the path in the configuration.\")\n",
    "        return # Exit if critical file is missing\n",
    "\n",
    "    df_active = df_vf[df_vf[VF_TOTAL_REG_COL] > 0].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "    df_active.loc[:, VF_BLOCK_ID_COL] = df_active[VF_BLOCK_ID_COL].astype(str).str.zfill(15)\n",
    "    df_active.loc[:, 'census_tract'] = df_active[VF_BLOCK_ID_COL].str[:11]\n",
    "\n",
    "    # Calculate estimated votes\n",
    "    # Handle cases where party columns might be missing (e.g., some states might not have NPP)\n",
    "    df_active.loc[:, 'dem_proportion'] = (df_active[VF_PARTY_DEM_COL] / df_active[VF_TOTAL_REG_COL]).fillna(0) if VF_PARTY_DEM_COL in df_active.columns else 0\n",
    "    df_active.loc[:, 'rep_proportion'] = (df_active[VF_PARTY_REP_COL] / df_active[VF_TOTAL_REG_COL]).fillna(0) if VF_PARTY_REP_COL in df_active.columns else 0\n",
    "    \n",
    "    if VF_GENERAL_2020_VOTES_COL not in df_active.columns:\n",
    "        logging.error(f\"Specified general election turnout column '{VF_GENERAL_2020_VOTES_COL}' not found in voter file. Cannot estimate votes.\")\n",
    "        return\n",
    "\n",
    "    df_active.loc[:, 'Estimated_Dem_Votes_2020'] = (df_active[VF_GENERAL_2020_VOTES_COL] * df_active['dem_proportion']).fillna(0)\n",
    "    df_active.loc[:, 'Estimated_Rep_Votes_2020'] = (df_active[VF_GENERAL_2020_VOTES_COL] * df_active['rep_proportion']).fillna(0)\n",
    "\n",
    "    # Aggregate voter data to tracts\n",
    "    logging.info(\"Aggregating voter data to Census Tracts...\")\n",
    "    tract_voter_agg_cols = {\n",
    "        'total_reg_tract': (VF_TOTAL_REG_COL, 'sum'),\n",
    "        'voted_general_2020_tract': (VF_GENERAL_2020_VOTES_COL, 'sum'),\n",
    "        'estimated_dem_votes_tract': ('Estimated_Dem_Votes_2020', 'sum'),\n",
    "        'estimated_rep_votes_tract': ('Estimated_Rep_Votes_2020', 'sum')\n",
    "    }\n",
    "    if VF_PARTY_DEM_COL in df_active.columns: tract_voter_agg_cols['party_dem_tract'] = (VF_PARTY_DEM_COL, 'sum')\n",
    "    if VF_PARTY_REP_COL in df_active.columns: tract_voter_agg_cols['party_rep_tract'] = (VF_PARTY_REP_COL, 'sum')\n",
    "    if VF_PARTY_NPP_COL in df_active.columns: tract_voter_agg_cols['party_npp_tract'] = (VF_PARTY_NPP_COL, 'sum')\n",
    "    \n",
    "    # Create the aggregation dictionary from the valid columns\n",
    "    final_tract_voter_agg_dict = {new_col: (orig_col, agg_func) for new_col, (orig_col, agg_func) in tract_voter_agg_cols.items()}\n",
    "\n",
    "    tract_voter_agg = df_active.groupby('census_tract').agg(**final_tract_voter_agg_dict).reset_index()\n",
    "    logging.info(f\"Aggregated voter data for {len(tract_voter_agg)} tracts.\")\n",
    "\n",
    "    # 2. Load CVAP Data\n",
    "    logging.info(f\"Loading CVAP data: {CVAP_FILE_PATH}\")\n",
    "    try:\n",
    "        df_cvap = pd.read_csv(CVAP_FILE_PATH)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"CVAP file not found at {CVAP_FILE_PATH}. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    df_cvap.loc[:, CVAP_BLOCK_ID_COL] = df_cvap[CVAP_BLOCK_ID_COL].astype(str).str.zfill(15)\n",
    "    if CVAP_TOTAL_COL not in df_cvap.columns:\n",
    "        logging.error(f\"Total CVAP column '{CVAP_TOTAL_COL}' not found in CVAP file.\")\n",
    "        return\n",
    "    df_cvap = df_cvap[df_cvap[CVAP_TOTAL_COL] > 0].copy()\n",
    "    df_cvap.loc[:, 'census_tract'] = df_cvap[CVAP_BLOCK_ID_COL].str[:11]\n",
    "\n",
    "    logging.info(\"Aggregating CVAP data to Census Tracts and calculating percentages...\")\n",
    "    cvap_agg_base = {CVAP_TOTAL_COL: 'sum', CVAP_WHITE_COL: 'sum'} # Base always needed\n",
    "    for group_key in MINORITY_GROUPS_FOR_ANALYSIS.keys():\n",
    "        cvap_col_for_group = CVAP_COLUMN_MAP_FOR_GROUPS.get(group_key)\n",
    "        if cvap_col_for_group and cvap_col_for_group in df_cvap.columns:\n",
    "            cvap_agg_base[cvap_col_for_group] = 'sum'\n",
    "        else:\n",
    "            logging.warning(f\"CVAP column for group '{group_key}' (expected: {cvap_col_for_group}) not found in CVAP file. It will be missing from tract CVAP data.\")\n",
    "\n",
    "    tract_cvap = df_cvap.groupby('census_tract').agg(\n",
    "        {k: v for k,v in cvap_agg_base.items() if k in df_cvap.columns} # Only aggregate existing columns\n",
    "    ).reset_index()\n",
    "\n",
    "    for group_key, pct_col_name_target in MINORITY_GROUPS_FOR_ANALYSIS.items():\n",
    "        raw_cvap_col_for_group = CVAP_COLUMN_MAP_FOR_GROUPS.get(group_key)\n",
    "        if raw_cvap_col_for_group and raw_cvap_col_for_group in tract_cvap.columns and \\\n",
    "           CVAP_TOTAL_COL in tract_cvap.columns and tract_cvap[CVAP_TOTAL_COL].nunique() > 0 : # Check for non-zero total\n",
    "            tract_cvap[pct_col_name_target] = (tract_cvap[raw_cvap_col_for_group] / tract_cvap[CVAP_TOTAL_COL].replace(0, np.nan) * 100).fillna(0)\n",
    "        else:\n",
    "            logging.warning(f\"Could not calculate {pct_col_name_target}. Raw CVAP column '{raw_cvap_col_for_group}' or total '{CVAP_TOTAL_COL}' missing or all zeros in tract_cvap.\")\n",
    "            tract_cvap[pct_col_name_target] = 0.0\n",
    "    logging.info(f\"Aggregated CVAP data for {len(tract_cvap)} tracts.\")\n",
    "\n",
    "    # 3. Load Shapefiles\n",
    "    logging.info(f\"Loading Tract shapefile: {TRACT_SHAPEFILE_PATH}\")\n",
    "    gdf_tract_shapes = None\n",
    "    try:\n",
    "        gdf_tract_shapes = gpd.read_file(TRACT_SHAPEFILE_PATH)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not load Tract shapefile: {e}. Ensure path is correct and all shapefile components (.shp, .dbf, .shx, etc.) are present.\")\n",
    "        return\n",
    "    gdf_tract_shapes.loc[:, 'census_tract'] = gdf_tract_shapes[TRACT_SHP_ID_COL].astype(str).str.zfill(11)\n",
    "\n",
    "    logging.info(f\"Loading District shapefile: {DISTRICT_SHAPEFILE_PATH}\")\n",
    "    gdf_districts = None\n",
    "    try:\n",
    "        gdf_districts = gpd.read_file(DISTRICT_SHAPEFILE_PATH)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not load District shapefile: {e}. Ensure path is correct and all components are present.\")\n",
    "        return\n",
    "    \n",
    "    if DISTRICT_SHP_ID_COL not in gdf_districts.columns:\n",
    "        logging.error(f\"District ID column '{DISTRICT_SHP_ID_COL}' not found in district shapefile. Available columns: {gdf_districts.columns.tolist()}\")\n",
    "        # Attempt to find a common alternative if 'DISTRICT' is specified but not found\n",
    "        if DISTRICT_SHP_ID_COL == 'DISTRICT':\n",
    "            common_alternatives = ['DISTRICTNO', 'DIST_NUM', 'CD118FP', 'GEOID'] # Add more as needed\n",
    "            for alt_col in common_alternatives:\n",
    "                if alt_col in gdf_districts.columns:\n",
    "                    logging.warning(f\"'{DISTRICT_SHP_ID_COL}' not found, using alternative '{alt_col}' as district ID.\")\n",
    "                    DISTRICT_SHP_ID_COL = alt_col # Update global for the rest of the script\n",
    "                    break\n",
    "            else: # If no alternative found\n",
    "                 logging.error(\"No suitable alternative district ID column found. Please check shapefile attributes.\")\n",
    "                 return\n",
    "        else: # If a specific non-DISTRICT column was given and not found\n",
    "            return\n",
    "\n",
    "    gdf_districts.loc[:, DISTRICT_SHP_ID_COL] = gdf_districts[DISTRICT_SHP_ID_COL].astype(str).str.replace(r'\\D+', '', regex=True).fillna('0').astype(int)\n",
    "\n",
    "\n",
    "    # 4. Merge Data\n",
    "    logging.info(\"Merging tract-level data...\")\n",
    "    gdf_tracts_data = pd.merge(tract_voter_agg, tract_cvap, on='census_tract', how='left')\n",
    "    gdf_tracts_merged = pd.merge(gdf_tract_shapes, gdf_tracts_data, on='census_tract', how='inner')\n",
    "    logging.info(f\"Merged data for {len(gdf_tracts_merged)} tracts with geometries.\")\n",
    "    if gdf_tracts_merged.empty:\n",
    "        logging.error(\"No tracts remained after merging all data sources. Check join keys ('census_tract') and data content.\")\n",
    "        return\n",
    "\n",
    "    # 5. Spatial Join\n",
    "    logging.info(\"Performing spatial join (assigning tracts to districts)...\")\n",
    "    if gdf_districts.crs != gdf_tracts_merged.crs:\n",
    "        logging.info(f\"Reprojecting tracts from {gdf_tracts_merged.crs} to {gdf_districts.crs} for spatial join.\")\n",
    "        gdf_tracts_merged = gdf_tracts_merged.to_crs(gdf_districts.crs)\n",
    "    \n",
    "    gdf_tracts_with_district = gpd.sjoin(gdf_tracts_merged, gdf_districts[[DISTRICT_SHP_ID_COL, 'geometry']],\n",
    "                                         how='left', predicate='intersects') # Using default lsuffix, rsuffix\n",
    "    # Clean up suffixes if they occur (e.g. index_right) before dropna\n",
    "    if 'index_right' in gdf_tracts_with_district.columns: # Common suffix from sjoin\n",
    "        gdf_tracts_with_district.rename(columns={'index_right': 'sjoin_index'}, inplace=True)\n",
    "\n",
    "    gdf_tracts_with_district.dropna(subset=[DISTRICT_SHP_ID_COL], inplace=True)\n",
    "    gdf_tracts_with_district = gdf_tracts_with_district.drop_duplicates(subset=['census_tract'], keep='first')\n",
    "    gdf_tracts_with_district.loc[:, DISTRICT_SHP_ID_COL] = gdf_tracts_with_district[DISTRICT_SHP_ID_COL].astype(int)\n",
    "    logging.info(f\"Assigned {len(gdf_tracts_with_district)} tracts to districts.\")\n",
    "    if gdf_tracts_with_district.empty:\n",
    "        logging.error(\"No tracts were successfully assigned to districts. Check CRS and spatial join predicate.\")\n",
    "        return\n",
    "\n",
    "    # 6. Aggregate Data to District Level\n",
    "    logging.info(\"Aggregating data to district level...\")\n",
    "    # Define base aggregation dictionary\n",
    "    agg_to_dist_base = {\n",
    "        'total_reg_tract': 'sum', \n",
    "        'voted_general_2020_tract': 'sum', \n",
    "        'estimated_dem_votes_tract': 'sum',\n",
    "        'estimated_rep_votes_tract': 'sum',\n",
    "        CVAP_TOTAL_COL: 'sum' # Raw total CVAP count\n",
    "    }\n",
    "    # Add optional party registration sums\n",
    "    if 'party_dem_tract' in gdf_tracts_with_district.columns: agg_to_dist_base['party_dem_tract'] = 'sum'\n",
    "    if 'party_rep_tract' in gdf_tracts_with_district.columns: agg_to_dist_base['party_rep_tract'] = 'sum'\n",
    "    if 'party_npp_tract' in gdf_tracts_with_district.columns: agg_to_dist_base['party_npp_tract'] = 'sum'\n",
    "\n",
    "    # Add raw CVAP counts for specified minority groups for district aggregation\n",
    "    for group_key in MINORITY_GROUPS_FOR_ANALYSIS.keys():\n",
    "        raw_cvap_col_for_group = CVAP_COLUMN_MAP_FOR_GROUPS.get(group_key)\n",
    "        if raw_cvap_col_for_group and raw_cvap_col_for_group in gdf_tracts_with_district.columns:\n",
    "            agg_to_dist_base[raw_cvap_col_for_group] = 'sum'\n",
    "\n",
    "    # Filter for columns that actually exist in the dataframe to avoid KeyErrors\n",
    "    final_agg_to_dist_dict = {k: v for k, v in agg_to_dist_base.items() if k in gdf_tracts_with_district.columns}\n",
    "    if not final_agg_to_dist_dict:\n",
    "        logging.error(\"No columns available for district aggregation. Check previous steps.\")\n",
    "        return\n",
    "\n",
    "    district_aggregates = gdf_tracts_with_district.groupby(DISTRICT_SHP_ID_COL).agg(final_agg_to_dist_dict).reset_index()\n",
    "    gdf_districts_final = pd.merge(gdf_districts, district_aggregates, on=DISTRICT_SHP_ID_COL, how='left')\n",
    "    gdf_districts_final.fillna(0, inplace=True)\n",
    "\n",
    "    # Calculate overall district CVAP percentages for context and export\n",
    "    for group_key in MINORITY_GROUPS_FOR_ANALYSIS.keys():\n",
    "        dist_pct_col_name = f'pct_cvap_{group_key}_dist'\n",
    "        raw_cvap_col_for_group = CVAP_COLUMN_MAP_FOR_GROUPS.get(group_key)\n",
    "        if raw_cvap_col_for_group and raw_cvap_col_for_group in gdf_districts_final.columns and \\\n",
    "           CVAP_TOTAL_COL in gdf_districts_final.columns and gdf_districts_final[CVAP_TOTAL_COL].sum() > 0:\n",
    "            gdf_districts_final[dist_pct_col_name] = (gdf_districts_final[raw_cvap_col_for_group] / gdf_districts_final[CVAP_TOTAL_COL].replace(0, np.nan) * 100).fillna(0)\n",
    "        else:\n",
    "            gdf_districts_final[dist_pct_col_name] = 0.0\n",
    "            logging.warning(f\"Could not calculate overall district {group_key} CVAP %. Check columns: {raw_cvap_col_for_group}, {CVAP_TOTAL_COL} in gdf_districts_final\")\n",
    "\n",
    "    # 7. Calculate Gerrymandering Metrics\n",
    "    pp_scores_df = calculate_polsby_popper(gdf_districts_final, DISTRICT_SHP_ID_COL, TARGET_CRS_EPSG)\n",
    "    gdf_districts_final = pd.merge(gdf_districts_final, pp_scores_df, on=DISTRICT_SHP_ID_COL, how='left')\n",
    "\n",
    "    efficiency_gap_value, eg_details_df = calculate_efficiency_gap(\n",
    "        gdf_districts_final, DISTRICT_SHP_ID_COL,\n",
    "        'estimated_dem_votes_tract', 'estimated_rep_votes_tract' # These are the aggregated sums\n",
    "    )\n",
    "\n",
    "    # Prepare pct columns for community concentration function\n",
    "    # These are the 'pct_black', 'pct_hispanic' etc. columns on the *tract* level data\n",
    "    minority_pct_cols_for_func = {key: val for key, val in MINORITY_GROUPS_FOR_ANALYSIS.items() if val in gdf_tracts_with_district.columns}\n",
    "    if not minority_pct_cols_for_func:\n",
    "        logging.warning(\"No valid CVAP percentage columns found in tract data for community concentration. Skipping.\")\n",
    "        comm_conc_summary_df = pd.DataFrame()\n",
    "        comm_conc_details_df = pd.DataFrame()\n",
    "    else:\n",
    "        comm_conc_summary_df, comm_conc_details_df = calculate_community_concentration(\n",
    "            gdf_tracts_with_district, DISTRICT_SHP_ID_COL,\n",
    "            minority_pct_cols_for_func,\n",
    "            COMMUNITY_THRESHOLD_PCT\n",
    "        )\n",
    "\n",
    "    # 8. Output Results\n",
    "    logging.info(\"\\n--- METRIC SUMMARY ---\")\n",
    "    logging.info(f\"State: {STATE_NAME}\")\n",
    "\n",
    "    if 'polsby_popper' in gdf_districts_final.columns:\n",
    "        logging.info(\"\\nPolsby-Popper Scores:\\n\" + gdf_districts_final[[DISTRICT_SHP_ID_COL, 'polsby_popper']].round(4).to_string(index=False))\n",
    "    else:\n",
    "        logging.warning(\"Polsby-Popper scores not available in final district data.\")\n",
    "\n",
    "    logging.info(f\"\\nEfficiency Gap: {efficiency_gap_value:.4f}\")\n",
    "    if efficiency_gap_value < -0.0001: logging.info(f\"Interpretation: Favors Republicans by {abs(efficiency_gap_value):.2%}\")\n",
    "    elif efficiency_gap_value > 0.0001: logging.info(f\"Interpretation: Favors Democrats by {efficiency_gap_value:.2%}\")\n",
    "    else: logging.info(\"Interpretation: Neutral\")\n",
    "\n",
    "    logging.info(\"\\nCommunity Concentration Summary:\")\n",
    "    if not comm_conc_summary_df.empty:\n",
    "        summary_display = comm_conc_summary_df.copy()\n",
    "        if 'Concentration Score (Sum Sq Shares)' in summary_display.columns:\n",
    "            summary_display['Concentration Score (Sum Sq Shares)'] = summary_display['Concentration Score (Sum Sq Shares)'].map('{:.3f}'.format)\n",
    "        if 'Baseline Even Spread' in summary_display.columns:\n",
    "            summary_display['Baseline Even Spread'] = summary_display['Baseline Even Spread'].map('{:.3f}'.format)\n",
    "        logging.info(\"\\n\" + summary_display[['Minority Group', 'Total High-Conc Tracts Statewide', 'Concentration Score (Sum Sq Shares)', 'Baseline Even Spread', 'Interpretation']].to_string(index=False))\n",
    "    else:\n",
    "        logging.info(\"Community concentration analysis was skipped or produced no results.\")\n",
    "\n",
    "    # Save detailed tables\n",
    "    if not eg_details_df.empty:\n",
    "        eg_details_df.to_csv(os.path.join(RESULTS_DIR, f\"{STATE_ABBR}_efficiency_gap_details.csv\"), index=False)\n",
    "    \n",
    "    if not comm_conc_details_df.empty:\n",
    "        # Merge comm_conc_details_df with overall district CVAP % for the contextual view\n",
    "        context_cols_to_select = [DISTRICT_SHP_ID_COL] + \\\n",
    "                                 [f'pct_cvap_{gk}_dist' for gk in MINORITY_GROUPS_FOR_ANALYSIS.keys() if f'pct_cvap_{gk}_dist' in gdf_districts_final.columns]\n",
    "        \n",
    "        # Ensure comm_conc_details_df has the district ID column with the correct name for merging\n",
    "        if DISTRICT_SHP_ID_COL not in comm_conc_details_df.columns and 'DISTRICT' in comm_conc_details_df.columns:\n",
    "            comm_conc_details_df.rename(columns={'DISTRICT': DISTRICT_SHP_ID_COL}, inplace=True)\n",
    "        \n",
    "        if DISTRICT_SHP_ID_COL in comm_conc_details_df.columns and DISTRICT_SHP_ID_COL in gdf_districts_final.columns:\n",
    "            contextual_comm_conc_df = pd.merge(\n",
    "                gdf_districts_final[context_cols_to_select],\n",
    "                comm_conc_details_df,\n",
    "                on=DISTRICT_SHP_ID_COL,\n",
    "                how='left'\n",
    "            )\n",
    "            contextual_comm_conc_df.to_csv(os.path.join(RESULTS_DIR, f\"{STATE_ABBR}_community_concentration_details_contextual.csv\"), index=False)\n",
    "        else:\n",
    "            logging.warning(f\"Could not create contextual community concentration CSV due to missing district ID column ('{DISTRICT_SHP_ID_COL}') in one of the DataFrames.\")\n",
    "\n",
    "\n",
    "    # --- Create State Summary JSON for Frontend ---\n",
    "    state_summary_data = {\n",
    "        \"state_abbr\": STATE_ABBR,\n",
    "        \"state_name\": STATE_NAME,\n",
    "        \"efficiency_gap\": round(efficiency_gap_value, 4) if pd.notna(efficiency_gap_value) else None,\n",
    "        \"efficiency_gap_interpretation\": (\n",
    "            f\"Favors Republicans by {abs(efficiency_gap_value):.2%}\" if efficiency_gap_value < -0.0001 else\n",
    "            f\"Favors Democrats by {efficiency_gap_value:.2%}\" if efficiency_gap_value > 0.0001 else\n",
    "            \"Neutral\"\n",
    "        ),\n",
    "        \"community_concentration\": []\n",
    "    }\n",
    "    if not comm_conc_summary_df.empty:\n",
    "        for _, row in comm_conc_summary_df.iterrows():\n",
    "            state_summary_data[\"community_concentration\"].append({\n",
    "                \"group\": row[\"Minority Group\"],\n",
    "                \"score\": round(row[\"Concentration Score (Sum Sq Shares)\"], 3) if pd.notna(row[\"Concentration Score (Sum Sq Shares)\"]) else None,\n",
    "                \"baseline\": round(row[\"Baseline Even Spread\"], 3) if pd.notna(row[\"Baseline Even Spread\"]) else None,\n",
    "                \"interpretation\": row[\"Interpretation\"],\n",
    "                \"total_high_conc_tracts\": row[\"Total High-Conc Tracts Statewide\"]\n",
    "            })\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, f\"{STATE_ABBR}_state_summary.json\"), 'w') as f:\n",
    "        json.dump(state_summary_data, f, indent=4)\n",
    "    logging.info(f\"Saved state summary JSON to {RESULTS_DIR}\")\n",
    "\n",
    "\n",
    "    # Save final GeoDataFrame with scores for potential front-end use\n",
    "    cols_to_export = [DISTRICT_SHP_ID_COL, 'geometry']\n",
    "    if 'polsby_popper' in gdf_districts_final.columns: cols_to_export.append('polsby_popper')\n",
    "    if 'estimated_dem_votes_tract' in gdf_districts_final.columns: cols_to_export.append('estimated_dem_votes_tract')\n",
    "    if 'estimated_rep_votes_tract' in gdf_districts_final.columns: cols_to_export.append('estimated_rep_votes_tract')\n",
    "    if CVAP_TOTAL_COL in gdf_districts_final.columns: cols_to_export.append(CVAP_TOTAL_COL)\n",
    "\n",
    "    for group_key in MINORITY_GROUPS_FOR_ANALYSIS.keys():\n",
    "        dist_pct_col = f'pct_cvap_{group_key}_dist'\n",
    "        if dist_pct_col in gdf_districts_final.columns:\n",
    "            cols_to_export.append(dist_pct_col)\n",
    "    \n",
    "    final_export_cols = [col for col in cols_to_export if col in gdf_districts_final.columns]\n",
    "    if 'geometry' in gdf_districts_final.columns and DISTRICT_SHP_ID_COL in gdf_districts_final.columns:\n",
    "        gdf_districts_final[final_export_cols].to_file(os.path.join(RESULTS_DIR, f\"{STATE_ABBR}_districts_metrics.geojson\"), driver=\"GeoJSON\")\n",
    "        logging.info(f\"Saved detailed district metrics to GeoJSON in {RESULTS_DIR}\")\n",
    "    else:\n",
    "        logging.warning(\"Could not save GeoJSON: 'geometry' or district ID column missing in final district data.\")\n",
    "\n",
    "    # Plot compactness scores\n",
    "    if 'polsby_popper' in gdf_districts_final.columns and 'geometry' in gdf_districts_final.columns:\n",
    "        try:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "            gdf_districts_final.plot(column='polsby_popper', ax=ax, legend=True,\n",
    "                                     legend_kwds={'label': \"Polsby-Popper Score\", 'orientation': \"horizontal\"},\n",
    "                                     missing_kwds={\"color\": \"lightgrey\", \"label\": \"Missing data\"})\n",
    "            ax.set_title(f\"{STATE_NAME} Congressional District Compactness (Polsby-Popper)\\nTarget CRS for calc: {TARGET_CRS_EPSG}\")\n",
    "            plt.savefig(os.path.join(RESULTS_DIR, f\"{STATE_ABBR}_compactness_map.png\"))\n",
    "            logging.info(f\"Saved compactness map to {RESULTS_DIR}\")\n",
    "            plt.close(fig) # Close the figure to free memory\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not plot or save compactness map: {e}\")\n",
    "    else:\n",
    "        logging.warning(\"Could not plot compactness map: 'polsby_popper' or 'geometry' column missing.\")\n",
    "\n",
    "    logging.info(f\"Analysis for {STATE_NAME} complete. Results in {RESULTS_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This allows the script to be run from the command line.\n",
    "    # For use in a Jupyter notebook, you would typically call main() directly\n",
    "    # after setting the configuration variables at the top.\n",
    "    # Example:\n",
    "    # STATE_ABBR = \"CA\"; STATE_NAME = \"California\"; ... (set all configs) ...\n",
    "    # main()\n",
    "    \n",
    "    # If you want to make it runnable with arguments (more advanced setup):\n",
    "    # import argparse\n",
    "    # parser = argparse.ArgumentParser(description=f\"Run gerrymandering analysis for {STATE_NAME}.\")\n",
    "    # You would then parse arguments to override default config values if needed.\n",
    "    # For now, direct modification of the config section is the primary method.\n",
    "    \n",
    "    # To run directly for the state configured at the top:\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
